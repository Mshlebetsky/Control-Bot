import csv
import multiprocessing
import os
import time
from tqdm import tqdm
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from multiprocessing import Process, Manager
from selenium.common.exceptions import NoSuchElementException, WebDriverException

# --------------------------------------------
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
# --------------------------------------------
NUM_BROWSERS = 12
CSV_FILE = "rulate_books.csv"
BASE_URL = "https://tl.rulate.ru/search/index/t//cat/0/rate_min/0/rate_max/5/rate_quality_min/0/rate_quality_max/5/s_lang/0/t_lang/0/adult/0/type/0/remove_machinelate/0/sort/4/genres_cond/0/tags_cond/0/fandoms_cond/0/fandoms_ex_all/0/n_chapters/0/n_chapters_max//atmosphere/0/Book_page/"

# --------------------------------------------
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
# --------------------------------------------
def collect_data(driver):
    data = {}
    try:
        book_pages = driver.find_element(By.CLASS_NAME, "search-results").find_elements(By.TAG_NAME, "li")
    except NoSuchElementException:
        return data

    for book in book_pages:
        tokens = {"–ø–æ—Å–ª–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å:": "", "—Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞:": "", "–∂–∞–Ω—Ä—ã:": "", "—Ç—ç–≥–∏:": "", "—Ñ–∞–Ω–¥–æ–º—ã:": ""}
        try:
            title = book.find_element(By.CLASS_NAME, "book-tooltip.tooltipstered").text
        except:
            continue

        if "/" in title:
            original_title, translated_title = title.split("/", 1)
        else:
            original_title, translated_title = title, ""

        try:
            link = book.find_element(By.CLASS_NAME, "book-tooltip.tooltipstered").find_element(By.TAG_NAME, "a").get_attribute("href")
        except:
            link = "–Ω–µ—Ç —Å—Å—ã–ª–∫–∏"

        labels = book.find_elements(By.CLASS_NAME, "label")
        chapters = labels[0].text.split("/")[0] if len(labels) > 0 else "0"
        total_pages = labels[1].text if len(labels) > 1 else "0"
        rating = labels[2].text if len(labels) > 2 else "0"
        likes = labels[-2].text if len(labels) >= 2 else "0"

        small_category = book.find_element(By.CLASS_NAME, "cat").text if book.find_elements(By.CLASS_NAME, "cat") else "–Ω–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"
        author_elements = book.find_elements(By.CLASS_NAME, "user.user-inactive")
        author_name = author_elements[0].text if author_elements else "–Ω–µ—Ç –∞–≤—Ç–æ—Ä–∞"
        author_link = author_elements[0].get_attribute("href") if author_elements else "–Ω–µ—Ç –∞–≤—Ç–æ—Ä–∞"

        description = ""
        try:
            for p in book.find_element(By.CLASS_NAME, "meta").find_elements(By.TAG_NAME, "p"):
                for token in tokens:
                    if p.text.startswith(token):
                        tokens[token] = p.text.replace(token, "").strip()
                        break
                else:
                    if ":" not in p.text[:25]:
                        description = p.text
        except:
            pass

        data[original_title] = [
            title, translated_title, link, chapters, total_pages, rating, likes,
            small_category, author_name, author_link,
            tokens["–ø–æ—Å–ª–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å:"],
            tokens["–∂–∞–Ω—Ä—ã:"], tokens["—Ç—ç–≥–∏:"], tokens["—Ñ–∞–Ω–¥–æ–º—ã:"],
            tokens["—Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞:"], description
        ]

    return data


# --------------------------------------------
# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
# --------------------------------------------
def worker(urls, existing_links, lock, process_id):
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    driver = webdriver.Chrome(options=options)

    # —Å–æ–∑–¥–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –∫–æ–ø–∏—é —Å—Å—ã–ª–æ–∫ (—á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫)
    local_existing_links = set(existing_links.keys())

    for url in tqdm(urls, desc=f"PID {os.getpid()}", position=process_id):
        try:
            driver.get(url)
            time.sleep(1)
            books = collect_data(driver)
            with lock:
                with open("rulate_books.csv", "a", newline="", encoding="utf-8") as f:
                    writer = csv.writer(f)
                    for key, values in books.items():
                        link = values[2]
                        if not link or link in local_existing_links:
                            continue
                        writer.writerow([key] + values)
                        existing_links[link] = True  # –¥–æ–±–∞–≤–ª—è–µ–º –≤ –º–µ–Ω–µ–¥–∂–µ—Ä
                        local_existing_links.add(link)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ {os.getpid()}: {e}")
            continue

    driver.quit()




# --------------------------------------------
# –ì–ª–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞
# --------------------------------------------
if __name__ == "__main__":
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    temp_driver = webdriver.Chrome()
    temp_driver.get(BASE_URL + "0")
    time.sleep(2)
    total_pages = temp_driver.find_element(By.CLASS_NAME, "span8").find_element(By.TAG_NAME, "h3").text.split(" ")[1]
    total_pages = int(total_pages) // 20 + 1
    temp_driver.quit()

    print(f"üìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")

    urls = [f"{BASE_URL}{page}" for page in range(1, total_pages + 1)]
    # chunk_size = len(urls) // NUM_BROWSERS
    num_processes = NUM_BROWSERS
    chunk_size = len(urls) // num_processes
    url_chunks = [urls[i:i + chunk_size] for i in range(0, len(urls), chunk_size)]

    # –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –º–µ–∂–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–Ω–æ–≥–æ –æ–±–º–µ–Ω–∞
    manager = Manager()
    lock = manager.Lock()
    existing_links = manager.dict()  # –∏—Å–ø–æ–ª—å–∑—É–µ–º dict, —á—Ç–æ–±—ã —Ö—Ä–∞–Ω–∏—Ç—å —Å—Å—ã–ª–∫–∏ –∫–∞–∫ –∫–ª—é—á–∏


    # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Äî —Å–æ–∑–¥–∞—ë–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
    if not os.path.exists(CSV_FILE):
        with open(CSV_FILE, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow([
                "original_title", "title", "translated_title", "link", "chapters",
                "total_pages", "rating", "likes", "category", "author_name", "author_link",
                "last_activity", "genres", "tags", "fandoms", "translation_status", "description"
            ])

    # –°—Ç–∞—Ä—Ç—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å—ã
    processes = []
    for i in range(num_processes):
        urls_chunk = urls[i * chunk_size:(i + 1) * chunk_size]
        p = multiprocessing.Process(
            target=worker,
            args=(urls_chunk, existing_links, lock, i)
        )
        p.start()
        processes.append(p)
        print(f"üîπ –ü—Ä–æ—Ü–µ—Å—Å {p.pid} –∑–∞–ø—É—â–µ–Ω: {len(urls_chunk)} —Å—Ç—Ä–∞–Ω–∏—Ü")

    for p in processes:
        p.join()
    print("üéâ –í—Å–µ –ø—Ä–æ—Ü–µ—Å—Å—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã. –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ rulate_books.csv")
